# app/keyword/scraper.py

import time
import random
import urllib.parse
import re
import traceback
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# --- ë³´ì¡° í•¨ìˆ˜ë“¤ ---
CAFE_HOSTS = {"cafe.naver.com", "m.cafe.naver.com"}

def extract_cafe_ids(url: str):
    """ì¹´í˜ URLì—ì„œ ID ì¶”ì¶œ"""
    try: 
        p = urllib.parse.urlparse(url)
    except Exception: 
        return set()
    ids = set()
    qs = urllib.parse.parse_qs(p.query)
    for key in ("articleid", "clubid", "articleId", "clubId"):
        for val in qs.get(key, []):
            if val.isdigit(): 
                ids.add(val)
    for token in re.split(r"[/?=&]", p.path):
        if token.isdigit() and len(token) >= 4: 
            ids.add(token)
    return ids

def url_matches(target_url: str, candidate_url: str) -> bool:
    """ë‘ URLì´ ê°™ì€ ê²Œì‹œë¬¼ì¸ì§€ í™•ì¸"""
    try: 
        t, c = urllib.parse.urlparse(target_url), urllib.parse.urlparse(candidate_url)
    except Exception: 
        return False
    t_host, c_host = t.netloc.split(":")[0].lower(), c.netloc.split(":")[0].lower()
    if (t_host in CAFE_HOSTS) or (c_host in CAFE_HOSTS):
        t_ids, c_ids = extract_cafe_ids(target_url), extract_cafe_ids(candidate_url)
        if t_ids and c_ids and (t_ids & c_ids): 
            return True
        if t_ids and any(_id in candidate_url for _id in t_ids): 
            return True
    return candidate_url.startswith(target_url[: min(len(target_url), 60)])

def url_or_title_matches(target_url, target_title, candidate_link):
    """URL ë˜ëŠ” ì œëª©ìœ¼ë¡œ ë§¤ì¹­"""
    href = candidate_link.get_attribute("href") or ""
    link_text = candidate_link.text.strip()
    
    # URL ë§¤ì¹­
    if url_matches(target_url, href):
        return True
    
    # ì œëª© ë§¤ì¹­ (ê³µë°± ë“± ì •ê·œí™” í›„ ë¹„êµ)
    if target_title and link_text:
        normalized_target = "".join(target_title.split()).lower()
        normalized_link = "".join(link_text.split()).lower()
        if normalized_target in normalized_link or normalized_link in normalized_target:
            return True
    
    return False

def human_sleep(a=0.8, b=1.8):
    """ì‚¬ëŒì²˜ëŸ¼ ëœë¤ ëŒ€ê¸°"""
    time.sleep(random.uniform(a, b))

def is_valid_content_link(href):
    """'ì¼ë°˜ ì¸ê¸°ê¸€' ë¡œì§ì„ ìœ„í•œ ìœ íš¨í•œ ì½˜í…ì¸  ë§í¬ì¸ì§€ í™•ì¸"""
    if not href:
        return False
    
    exclude_patterns = [
        'javascript:', '#', '/search.naver', 'tab=', 'mode=', 'option=', 
        'query=', 'where=', 'sm=', 'ssc=', '/my.naver', 'help.naver', 
        'shopping.naver', 'terms.naver.com', 'nid.naver.com'
    ]
    href_lower = href.lower()
    if any(pattern in href_lower for pattern in exclude_patterns):
        return False
    
    include_patterns = [
        'blog.naver.com', 'cafe.naver.com', 'post.naver.com', 'kin.naver.com',
        'smartplace.naver', 'tv.naver.com', 'news.naver.com'
    ]
    if any(pattern in href for pattern in include_patterns):
        return True
    
    return False

# --- ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ ---
def run_check(keyword: str, post_url: str, post_title: str = None) -> tuple:
    """í‚¤ì›Œë“œë§ˆë‹¤ ë‹¤ë¥¸ êµ¬ì¡°ë¥¼ ë™ì ìœ¼ë¡œ íŒŒì•…í•˜ì—¬ ìˆœìœ„ ì¸¡ì •"""
    print(f"--- '{keyword}' ìˆœìœ„ í™•ì¸ ì‹œì‘ ---")
    
    options = webdriver.ChromeOptions()
    options.add_argument("--headless=new")
    options.add_argument("--window-size=1280,2200")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36")
    
    driver = None
    try:
        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
        q = urllib.parse.quote(keyword)
        
        print(f"[{keyword}] í†µí•©ê²€ìƒ‰ í˜ì´ì§€ ì ‘ê·¼ ì¤‘...")
        driver.get(f"https://search.naver.com/search.naver?query={q}")
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, "main_pack")))
        human_sleep()
        
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight)")
        time.sleep(1.5)

        all_sections = driver.find_elements(By.CSS_SELECTOR, ".sc_new, .view_wrap")
        print(f"[{keyword}] {len(all_sections)}ê°œ ì„¹ì…˜ ë°œê²¬")
        
        for section in all_sections:
            try:
                if not section.is_displayed() or section.size['height'] < 50:
                    continue
                
                section_title = extract_section_title(section, keyword)
                if "ì‡¼í•‘" in section_title or "ê´‘ê³ " in section_title:
                    continue

                print(f"[{keyword}] ì„¹ì…˜: '{section_title}' í™•ì¸ ì¤‘...")
                
                content_links = extract_content_links(section)
                if not content_links:
                    print(f"[{keyword}] '{section_title}'ì—ì„œ ì½˜í…ì¸  ë§í¬ë¥¼ ì°¾ì§€ ëª»í•¨")
                    continue
                
                print(f"[{keyword}] '{section_title}'ì—ì„œ {len(content_links)}ê°œ ì½˜í…ì¸  ë§í¬ ë°œê²¬")
                
                # ì¤‘ë³µ ì œê±°
                unique_links = []
                seen_hrefs = set()
                for link in content_links:
                    href = link.get_attribute('href')
                    if href not in seen_hrefs:
                        seen_hrefs.add(href)
                        unique_links.append(link)

                # ì´ ì„¹ì…˜ ë‚´ì—ì„œë§Œ ìˆœìœ„ ì¹´ìš´íŠ¸
                for rank, link in enumerate(unique_links, 1):
                    if url_or_title_matches(post_url, post_title, link):
                        print(f"âœ… [{keyword}] '{section_title}' ì„¹ì…˜ ë‚´ {rank}ìœ„ì—ì„œ ë°œê²¬!")
                        return (section_title, rank, section_title)  # ì„¹ì…˜ ë‚´ ìˆœìœ„ë§Œ ë°˜í™˜
            
            except Exception:
                continue
        
        print(f"âŒ [{keyword}] í†µí•©ê²€ìƒ‰ ê²°ê³¼ì—ì„œ URLì„ ì°¾ì§€ ëª»í•¨")
        return ("ë…¸ì¶œX", 999, None)

    except Exception as e:
        print(f"ğŸš¨ [{keyword}] ìˆœìœ„ í™•ì¸ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        traceback.print_exc()
        return ("í™•ì¸ ì‹¤íŒ¨", 999, None)
    finally:
        if driver:
            driver.quit()
        print(f"--- '{keyword}' ìˆœìœ„ í™•ì¸ ì™„ë£Œ ---\n")

def extract_section_title(section, keyword):
    """(ê°œì„ ) XPathì™€ í…ìŠ¤íŠ¸ ë¶„ì„ì„ í†µí•´ ë”ìš± ì •êµí•˜ê²Œ ì„¹ì…˜ ì œëª© ì¶”ì¶œ"""
    try:
        # 1. (ê¸°ì¡´ ë¡œì§) ìŠ¤ë§ˆíŠ¸ë¸”ë¡/ì‹ ê·œ ì„¹ì…˜ì˜ ëª…ì‹œì  í—¤ë“œë¼ì¸ ìš°ì„  íƒìƒ‰
        # ì„¹ì…˜ ë‚´ë¶€ì— h2, h3, a íƒœê·¸ ì¤‘ì— title í´ë˜ìŠ¤ë¥¼ ê°€ì§„ê²Œ ìˆëŠ”ì§€ í™•ì¸
        title_element = section.find_element(By.CSS_SELECTOR, "h2.title, h3.title, a.title, [class*='headline']")
        if title_element and title_element.text and len(title_element.text.strip()) > 1:
            title_text = title_element.text.strip()
            # "ë”ë³´ê¸°" ê°™ì€ ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°
            if "ë”ë³´ê¸°" in title_text:
                title_text = title_text.split("ë”ë³´ê¸°")[0].strip()
            return title_text

        # 2. (âœ¨ì‹ ê·œ ë¡œì§âœ¨) XPathë¥¼ ì‚¬ìš©í•´ ì„¹ì…˜ ë°”ë¡œ 'ì´ì „' ìš”ì†Œì—ì„œ ì œëª© íƒìƒ‰
        # "ê±´ê°•Â·ì˜í•™ ì¸ê¸°ê¸€" ê°™ì€ ì œëª©ì€ ì„¹ì…˜ ë°–ì— ìœ„ì¹˜í•˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.
        try:
            # ë°”ë¡œ ì§ì „ì˜ í˜•ì œ ìš”ì†Œ(div, h2 ë“±)ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
            prev_sibling = section.find_element(By.XPATH, "./preceding-sibling::*[1]")
            prev_text = prev_sibling.text.strip()
            if "ì¸ê¸°ê¸€" in prev_text and len(prev_text) < 50:
                 return prev_text
        except Exception:
            pass # ì´ì „ ìš”ì†Œê°€ ì—†ìœ¼ë©´ ê·¸ëƒ¥ ë„˜ì–´ê°‘ë‹ˆë‹¤.

        # 3. (ê¸°ì¡´ ë¡œì§ ê°œì„ ) ì„¹ì…˜ ë‚´ë¶€ í…ìŠ¤íŠ¸ì—ì„œ 'ì¸ê¸°ê¸€' íŒ¨í„´ ì°¾ê¸°
        section_text = section.text[:200]
        if "ì¸ê¸°ê¸€" in section_text:
            match = re.search(r'([\wÂ·\s]+)?ì¸ê¸°ê¸€', section_text)
            if match:
                title = match.group(0).strip()
                if len(title) < 30: return title
            return "ì¸ê¸°ê¸€"

        # 4. (ê¸°ì¡´ ë¡œì§) í´ë˜ìŠ¤ëª… ê¸°ë°˜ìœ¼ë¡œ ì„¹ì…˜ ì¢…ë¥˜ ì¶”ë¡ 
        class_name = section.get_attribute("class") or ""
        if "ad" in class_name or "power_link" in class_name: return "ê´‘ê³ "
        if "blog" in class_name: return "ë¸”ë¡œê·¸"
        if "cafe" in class_name: return "ì¹´í˜"

    except Exception:
        pass
    return "ê²€ìƒ‰ê²°ê³¼" # ìµœí›„ì˜ ë³´ë£¨


def extract_content_links(section):
    """ì‹¤ì œ ë³´ì´ëŠ” ê²Œì‹œë¬¼ ë§í¬ë§Œ ì •í™•íˆ ì¶”ì¶œ"""
    content_links = []
    
    try:
        # 0. ìŠ¤ë§ˆíŠ¸ë¸”ë¡ ìš°ì„  í™•ì¸ (ì¶”ê°€)
        post_text_containers = section.find_elements(By.CSS_SELECTOR, "div[class*='text-container']")
        if post_text_containers:
            for container in post_text_containers:
                try:
                    title_link = container.find_element(By.CSS_SELECTOR, "a[class*='text-title']")
                    content_links.append(title_link)
                except:
                    continue
            # ì‹¤ì œë¡œ ë§í¬ë¥¼ ì°¾ì•˜ì„ ë•Œë§Œ return
            if content_links:
                return content_links
        
        # 1. ì¼ë°˜ ì¸ê¸°ê¸€ ì²˜ë¦¬ (ì›ë³¸ ê·¸ëŒ€ë¡œ)
        # ë””ë²„ê¹…: ì„¹ì…˜ í…ìŠ¤íŠ¸ í™•ì¸
        section_text = section.text[:200] if section.text else ""
        if "ì¸ê¸°ê¸€" in section_text:
            print(f"  [ë””ë²„ê¹…] ì¸ê¸°ê¸€ ì„¹ì…˜ ë°œê²¬, í…ìŠ¤íŠ¸: {section_text[:100]}...")
        
        # ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ ë°©ì‹
        list_items = section.find_elements(By.CSS_SELECTOR, "li")
        
        # ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œì´ ì—†ìœ¼ë©´ ëª¨ë“  ë§í¬ ì‹œë„
        if not list_items:
            print(f"  [ë””ë²„ê¹…] li ìš”ì†Œ ì—†ìŒ, ëª¨ë“  a íƒœê·¸ ê²€ìƒ‰")
            all_links = section.find_elements(By.TAG_NAME, "a")
            for link in all_links:
                href = link.get_attribute("href") or ""
                text = link.text.strip()
                if ("blog.naver" in href or "cafe.naver" in href) and len(text) > 5:
                    print(f"    -> ë§í¬ ë°œê²¬: {text[:30]}...")
                    content_links.append(link)

        # 2. ë¦¬ìŠ¤íŠ¸ êµ¬ì¡°ê°€ ì•„ë‹Œ ê²½ìš°
        if not content_links:
            link_selectors = [
                "a.title_link",
                "a.api_txt_lines",
                "a.link_tit",
                "a.total_tit",
                "a.name",
                "a.dsc_link",
                "a[href*='blog.naver']",
                "a[href*='cafe.naver']",
            ]
            
            for selector in link_selectors:
                links = section.find_elements(By.CSS_SELECTOR, selector)
                for link in links:
                    if link.is_displayed() and link not in content_links:
                        href = link.get_attribute("href") or ""
                        text = link.text.strip()
                        
                        if is_valid_content_link(href) and len(text) > 5:
                            content_links.append(link)
        
        # 3. ê·¸ë˜ë„ ì—†ìœ¼ë©´ ëª¨ë“  ë§í¬ í™•ì¸ (ìµœí›„ ìˆ˜ë‹¨)
        if not content_links:
            all_links = section.find_elements(By.TAG_NAME, 'a')
            
            for link in all_links:
                if not link.is_displayed():
                    continue
                
                # ë„ˆë¬´ ì‘ì€ ë§í¬ ì œì™¸
                if link.size['height'] < 10 or link.size['width'] < 10:
                    continue
                
                href = link.get_attribute("href") or ""
                text = link.text.strip()
                
                # ìœ íš¨í•œ ì½˜í…ì¸  ë§í¬ì´ê³  ì¶©ë¶„í•œ í…ìŠ¤íŠ¸
                if is_valid_content_link(href) and len(text) > 5:
                    # UI ìš”ì†Œ ì œì™¸
                    if not any(skip in text for skip in ["ë”ë³´ê¸°", "ì„¤ì •", "ì˜µì…˜", "í•„í„°", "ì „ì²´"]):
                        if link not in content_links:
                            content_links.append(link)
        
    except Exception as e:
        print(f"ë§í¬ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
    
    return content_links